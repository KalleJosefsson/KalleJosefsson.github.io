
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> Final Project:NeRFs</title>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            text-align: center;
            margin-bottom: 50px;
        }
        h1 {
            font-size: 2.5em;
            color: #4CAF50;
        }
        h2, h3 {
            color: #4CAF50;
            margin-top: 40px;
        }
        p {
            font-size: 1.2em;
            line-height: 1.6em;
        }
        .image-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-top: 20px;
        }
        .image-container {
            text-align: center;
        }
        .image-container img {
            height: 200px; /* Set fixed height for all images */
            width: auto; /* Automatically adjust width to keep aspect ratio */
            border-radius: 10px;
        }
        .caption {
            margin-top: 10px;
            font-size: 14px;
            font-style: italic;
        }
        footer {
            text-align: center;
            margin-top: 50px;
            font-size: 0.8em;
            color: #777;
        }
    </style>
</head>
<body>
    <header>
        <h1>Final Project: NeRFs</h1>
    </header>


    <h2>
        Part 1. 2D NeRFs
    </h2>

    <p>
        For the first we are supposed to fit a Neural Field to a an image. This is done by creating a Multi Layer Perecptron with Sinusoidal Positional Encoding. The architecture of the 
        MLP is presented below.
    </p>
    <div class="image-container">
            <img src="images6/1arc.jpg" alt="Denoising begins">
            <div class="caption">MLP architecture for Part 1</div>
        </div>
    <p>
        As you can see in the image above, the MLP has 3 layers with each having 256 hidden dimensions. All of the hidden layers uses the ReLU activation function and the final output layer using the Sigmoid activation function instead.
        Throughout part 1 I used the Adam optimizer function and Mean Squared Error as loss. For this part I tried out some different settings for the hyper parameters to see what worked best. Below you will se the results. 
    </p>
    <div class="image-container">
            <img src="images6/fox_1.png" alt="Denoising begins">
            <div class="caption">1. Number of Encoding Functions: 10 & Learningrate: 1e-3 & Number Iterations: 1000</div>
        </div>
    <div class="image-container">
            <img src="images6/fox_2.png" alt="Denoising begins">
            <div class="caption">2. Number of Encoding Functions: 21 & Learningrate: 1e-2 & Number Iterations: 1000</div>
        </div>
    <div class="image-container">
            <img src="images6/fox_final.png" alt="Denoising begins">
            <div class="caption">3. Number of Encoding Functions: 10 & Learningrate: 1e-2 & Number Iterations: 2000</div>
        </div>

    <p>
        Below I present the PSNR (Peak Signal to Noise Ratio) and Loss plots which we used as the metrics to evaluate the performance of our model.
    </p>

     <div class="image-row">
        <div class="image-container">
            <img src="images6/psnr_1.png" alt="Edit at noise level 1">
            <div class="caption">1.PSNR and Loss for Hyperparameters settings 1</div>
        </div>
        <div class="image-container">
            <img src="images6/psnr_2.png" alt="Edit at noise level 3">
            <div class="caption">PSNR and Loss for Hyperparameters settings 2</div>
        </div>
         <div class="image-container">
            <img src="images6/psnr_final.png" alt="Edit at noise level 3">
            <div class="caption">PSNR and Loss for Hyperparameters settings 3</div>
        </div>
        
    </div>

    <p>
        As we can see the model which has 10 positional encoding function works great for both learning rates and number of iterations. The model converges faster with a higher learning rate
        but utlimately reaches the same PSNR. However if we increase the number of positional encodings the model does not work well with reproducing the image, however the PSNR is still high.
    </p>

    <p>
        I also tested the model on an image of Ali the famous boxer. This image had fewer pixels than the image of the fox, making it easier for the model to get a good results.
        As we can see below the results are in fact better.
        
    </p>

 

    <div class="image-container">
            <img src="images6/ali_results.png" alt="Edit at noise level 1">
            <div class="caption">Number of Encoding Functions: 10 & Learningrate: 1e-2 & Number Iterations: 1000</div>
        </div>
    <div class="image-container">
            <img src="images6/ali_psnr.png" alt="Edit at noise level 3">
            <div class="caption">PSNR and Loss for Ali</div>
        </div>

    <p> Training took around 40 seconds using the GPU however since the model almost converged after 500 iterations we could decrease training time to around 20 seconds which I consider very fast</p>
     <h2>
        Part 2: 3D NeRFs
    </h2>
    

    <p>
        To create the dataset, we start with a list of images, camera-to-world matrices, and the focal length, from which we compute the intrinsic matrix. The intrinsic matrix is essential as it encodes the camera's internal parameters, such as focal length and principal point offset, enabling accurate projection of 2D pixels into 3D rays within the scene. </p>

    <p>
        For each ray, we randomly select a pixel from the available images. This pixel, combined with its corresponding camera parameters, is projected into 3D space as a ray originating from the camera's position. These sampled rays simulate the viewing experience of a camera moving through the scene and allow the model to learn from a wide range of perspectives. Random sampling also introduces stochasticity, which helps prevent overfitting by exposing the model to diverse ray configurations during training.
        </p>

    <p>
        This ray sampling process ensures that the neural radiance field captures the spatial relationships and color information necessary to synthesize accurate novel views. Below is a visualization illustrating the ray sampling process from some of the cameras; showing all of them would make the representation too cluttered and unclear.
    </p>

    <div class="image-container">
            <img src="images6/3d_rays.png" alt="Edit at noise level 3">
            <div class="caption">Viser showing camera ray samples</div>
        </div>

    <p>
        Using the sampled rays, we perform discrete steps along each ray in 3D space. At each step, a small perturbation is applied to the sampling positions to improve robustness. This randomization ensures that the model does not overfit to a fixed sampling pattern and enhances its ability to generalize. At these discrete points, we query the neural radiance field to predict the RGB color and density (opacity) values.
    </p>

    <p>
        The outputs are aggregated using the volume rendering equation, which integrates the contributions of each point along the ray. The equation combines the radiance at each point with the density and transmittance (how much light passes through previous points) to create a realistic rendering of the scene. This approach allows for accurate modeling of lighting, shading, and occlusion effects.
    </p>
    
    <p>
        When training the network I did it over 3000 iterations with a batchsize of 10000. Again we use the Adam optimizer and MSE as loss function. Now we have a larger MLP for which the architecture is
        presented in the image below.
    </p>

    <div class="image-container">
            <img src="images6/2arc.png" alt="Edit at noise level 3">
            <div class="caption">Network architecture for part 2</div>
        </div>
<p>
    The novel views generated by NeRF are synthesized by rendering the scene from viewpoints not present in the training dataset. However, these views are often of lower quality due to limited information from the original dataset. Sparse or uneven camera coverage reduces the model's ability to infer unseen regions, leading to artifacts or blurring in these areas. Occluded regions (those not visible in any input image) are especially challenging, as the model must rely entirely on learned priors.
</p>

    <p>
        Below is a visualization of the progress during training, highlighting the iterative improvement of both standard and novel views.
    </p>

    <div class="image-row">
        <div class="image-container">
            <img src="images6/iter100.jpg" alt="Edit at noise level 1">
            <div class="caption">Iteration 100</div>
        </div>
        <div class="image-container">
            <img src="images6/iter300.jpg" alt="Edit at noise level 3">
            <div class="caption">Iteration 300</div>
        </div>
         <div class="image-container">
            <img src="images6/iter800.jpg" alt="Edit at noise level 3">
            <div class="caption">Iteration 800</div>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="images6/iter1800.jpg" alt="Edit at noise level 1">
            <div class="caption">Iteration 1800</div>
        </div>
        <div class="image-container">
            <img src="images6/iter2400.jpg" alt="Edit at noise level 3">
            <div class="caption">Iteration 2400</div>
        </div>
        
    </div>

    <p>
    I trained the model for around 10-15 minutes which made the model reach a Peak Signal to Noise Ratio of around 25 which I thought was good enough for the short training duration.
    However if I would increase the number of iterations and batchsize I could probavly increase the quality of the final 3D which was created using all the newly created novel views. Below is the PSNR plot for the training of the nerf. As you can see in the image
    the model has not yet converged which means by increasing the number of iterations would in fact yield an even better result
    </p>
<div class="image-container">
            <img src="images6/psnr_nerf.png" alt="Edit at noise level 3">
            <div class="caption">PSNR plot for 3D NeRF </div>
        </div>

    <p> The final 3D model constructed is presented in the GIF below</p>

    <div class="image-container">
            <img src="images6/training_color.gif" alt="Edit at noise level 3">
            <div class="caption">GIF showing 3D colorized reconstruction </div>
        </div>

    <p>
        Since NeRFs output both RGB values and density (or opacity), we can leverage this to achieve something fascinating: depth rendering. Instead of generating RGB values, we can modify the output to represent a single value between 0 and 1, indicating how far each pixel is from the camera.
    </p>

    <div class="image-container">
            <img src="images6/training.gif" alt="Edit at noise level 3">
            <div class="caption">GIF showing 3D depth reconstruction </div>
        </div>
      <footer>
        <p>© 2024 Kalle's Portfolio | Final Project: NeRFs</p>
        <p>Contact: <a href="mailto:kj00@berkeley.edu">kj00@berkeley.edu</a></p>
    </footer>
</body>
</html>
