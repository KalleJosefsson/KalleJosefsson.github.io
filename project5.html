<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: Diffusion Models</title>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            text-align: center;
            margin-bottom: 50px;
        }
        h1 {
            font-size: 2.5em;
            color: #4CAF50;
        }
        h2, h3 {
            color: #4CAF50;
            margin-top: 40px;
        }
        p {
            font-size: 1.2em;
            line-height: 1.6em;
        }
        .image-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-top: 20px;
        }
        .image-container {
            text-align: center;
        }
        .image-container img {
            height: 200px; /* Set fixed height for all images */
            width: auto; /* Automatically adjust width to keep aspect ratio */
            border-radius: 10px;
        }
        .caption {
            margin-top: 10px;
            font-size: 14px;
            font-style: italic;
        }
        footer {
            text-align: center;
            margin-top: 50px;
            font-size: 0.8em;
            color: #777;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models</h1>
    </header>

    <h2>Part 0. Setup</h2>
    <p>
        For this part, we set up the diffusion model with a random seed (185). This seed will be used throughout the project. In part A, we work with images of size 64x64 pixels, which might not be as sharp. In part B, the images improve significantly. Below, we show results with different inference steps.
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part1_1_n20.png" alt="20 Denoising Steps">
            <div class="caption">20 Denoising Steps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_1_n50.png" alt="50 Denoising Steps">
            <div class="caption">50 Denoising Steps</div>
        </div>
    </div>

    <h2>Part 1. Implementing the Forward Process</h2>
    <p>
        Adding noise progressively to an image is key in the diffusion process. This allows the model to learn how to reverse noise, recovering the clean image. Below are examples of the test image at various noise levels:
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part1_1_noise.png" alt="Image with 250 timesteps">
            <div class="caption">250 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_1_noise500.png" alt="Image with 500 timesteps">
            <div class="caption">500 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_1_noise750.png" alt="Image with 750 timesteps">
            <div class="caption">750 timesteps</div>
        </div>
    </div>

    <h2>Part 2. Denoising Using Classical Gaussian Blurring</h2>
    <p>
        Gaussian blurring was used to denoise the images. This method is not effective, as seen below:
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part1_2_250.png" alt="Blurred image at 250 timesteps">
            <div class="caption">250 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_2_500.png" alt="Blurred image at 500 timesteps">
            <div class="caption">500 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_2_750.png" alt="Blurred image at 750 timesteps">
            <div class="caption">750 timesteps</div>
        </div>
    </div>

    <h2>Part 3. One-Step Denoising Using the Diffusion Model</h2>
    <p>
        A U-Net estimates and removes Gaussian noise. The model considers the timestep as an input parameter, making it easier to determine the amount of noise to remove:
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part1_3_250.png" alt="Denoised image at 250 timesteps">
            <div class="caption">250 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_3_500.png" alt="Denoised image at 500 timesteps">
            <div class="caption">500 timesteps</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_3_750.png" alt="Denoised image at 750 timesteps">
            <div class="caption">750 timesteps</div>
        </div>
    </div>

    <h2>Part 4. Iterative Denoising Using the Diffusion Model</h2>
    <p>
        Iterative denoising works better than classical methods. To speed up the process, denoising is performed every 30 timesteps. Below is the progression of denoising and the final comparison to classical methods:
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part1_4_690.png" alt="Denoising begins">
            <div class="caption">Beginning of denoising</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_4_390.png" alt="Midway through denoising">
            <div class="caption">Middle of denoising</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_4_90.png" alt="Denoising completed">
            <div class="caption">End of denoising</div>
        </div>
    </div>
    <div class="image-container">
        <img src="images5/part1_4_finals.png" alt="Final comparison to classical methods">
        <div class="caption">Final comparison with classical denoising</div>
    </div>

        <h2>Part 5. Diffusion model sampling</h2>
    <p>
        In this part, we used the trained diffusion model to generate five random images. These images were sampled starting from pure noise and progressively denoised to create realistic outputs. This demonstrates the model's ability to synthesize diverse images based on the learned data distribution.
    </p>
    <div class="image-container">
        <img src="images5/part1_5.png" alt="Five Randomly Generated Images">
        <div class="caption">Five randomly generated images using the diffusion model</div>
    </div>
    <h2>Part 6. Classifier-Free Guidance (CFG)</h2>
    <p>
        To improve the quality of generated images, we implemented Classifier-Free Guidance (CFG). This technique combines a conditional noise estimate based on a text prompt with an unconditional noise estimate, weighted by a guidance parameter <code>γ</code>. The new noise estimate is computed as:
    </p>
    <blockquote>
        <code>ϵ = ϵ<sub>u</sub> + γ(ϵ<sub>c</sub> − ϵ<sub>u</sub>)</code>
    </blockquote>
    <p>
        By setting <code>γ > 1</code>, we achieve significantly higher-quality images at the expense of diversity. For <code>γ = 0</code>, the model generates unconditional results, and for <code>γ = 1</code>, it creates conditional results. The images below demonstrate the improvement in quality using CFG with <code>γ = 7</code>.
    </p>
    <div class="image-container">
        <img src="images5/part1_6.png" alt="Images generated using CFG with γ=7">
        <div class="caption">Images generated using Classifier-Free Guidance (γ=7)</div>
    </div>


  

        <h2>Part 7. Image-to-Image Translation</h2>
    <p>
        In this part, we explored the SDEdit algorithm to make creative edits to an image. By adding noise to a real image and denoising it using the diffusion model, we force the noisy image back onto the manifold of natural images. The extent of the edit depends on the noise level—higher noise levels lead to larger edits, while lower noise levels retain more of the original structure.
    </p>
    <p>
        Using the prompt <code>"a high quality photo"</code>, we applied this technique to the original test image at noise levels <code>[1, 3, 5, 7, 10, 20]</code>. The results demonstrate how the image transitions from noisy to more refined, with creative edits made by the model.
    </p>
    <h3>Results: Original Test Image</h3>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/part_1_711.png" alt="Edit at noise level 1">
            <div class="caption">First noise levels</div>
        </div>
        <div class="image-container">
            <img src="images5/part_1_712.png" alt="Edit at noise level 3">
            <div class="caption">Last noise levels</div>
        </div>
        
    </div>
    
    <h3>Results: Custom Test Images: Muhammed Ali</h3>
       <p>
        We repeated the process on two additional test images using the same noise levels. Below are the results for one of the custom images.
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/ali_1.png" alt="Edit at noise level 7">
            <div class="caption">Early noise levels</div>
        </div>
        <div class="image-container">
            <img src="images5/ali_2.png" alt="Edit at noise level 10">
            <div class="caption">Last noise levels</div>
        </div>
        <div class="image-container">
            <img src="images5/ali.webp" alt="Edit at noise level 20">
            <div class="caption">Original image</div>
        </div>
    </div>

  
 <div class="image-row">
    <div class="image-container">
        <img src="images5/goldengate_1.png" alt="Custom Test Image Edits">
        <div class="caption">Early noise levels</div>
    </div>
    <div class="image-container">
        <img src="images5/goldengate_2.png" alt="Custom Test Image Edits">
        <div class="caption">Last noise levels</div>
    </div>
    <div class="image-container">
        <img src="images5/goldengate.jpg" alt="Custom Test Image Edits">
        <div class="caption">Original image image</div>
    </div>
    </div>

      <h2>Part 7.1 Image-to-Image Translation for Hand-Drawn Images</h2>
    <p>
        In this part, we apply the same image-to-image translation technique but use hand-drawn images as input. Below is one hand-drawn image from the web and two images drawn by me.
    </p>

    <h3>Results: Web Image</h3>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/web_house.jpg" alt="Original Web Image">
            <div class="caption">Original Image</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_7_web.png" alt="Edited Web Image">
            <div class="caption">Edit at Different Noise Levels</div>
        </div>
    </div>

    <h3>Results: Custom Test Images - Hand-Drawn</h3>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/Snoopy.jpg" alt="Original Drawing of Snoopy">
            <div class="caption">Original Drawing of Snoopy</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_7_snoopy.png" alt="Edited Drawing of Snoopy">
            <div class="caption">Edit at Different Noise Levels</div>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="images5/One_love.jpg" alt="Original Hand-Drawn Image">
            <div class="caption">Original Drawing</div>
        </div>
        <div class="image-container">
            <img src="images5/part1_7_rut.png" alt="Edited Hand-Drawn Image">
            <div class="caption">Edit at Different Noise Levels</div>
        </div>
    </div>

        <h2>Part 7.2 Inpainting</h2>
    <p>
        In this section, we explore inpainting by following a method inspired by the RePaint paper. Given an image (<code>x<sub>orig</sub></code>) and a binary mask (<code>m</code>), we create a new image that preserves the original content where the mask is 0 and introduces new content where the mask is 1.
    </p>
    <p>
        To achieve this, we use the diffusion denoising loop with a small modification. At each denoising step, after obtaining <code>x<sub>t</sub></code>, we force it to retain the original pixels where <code>m</code> is 0, effectively leaving the masked areas intact. The process is defined as:
    </p>
    <blockquote>
        <code>x<sub>t</sub> ← m * x<sub>t</sub> + (1 − m) * forward(x<sub>orig</sub>, t)</code>
    </blockquote>
    <p>
        By iteratively applying this approach, the diffusion model fills in the masked area with new content while preserving the rest of the image. We applied this technique to inpaint the top of the Campanile using a custom mask.
    </p>
    
    <h3>Results: Inpainting the Campanile</h3>
    <div class="image-row">
       
        <div class="image-container">
            <img src="images5/part_1_7_mask.png" alt="Inpainting Mask for Campanile">
            <div class="caption">Stages</div>
        </div>
        <div class="image-container">
            <img src="images5/part_1_7_inpainted1.png" alt="Inpainted Image of the Campanile">
            <div class="caption">Inpainted Image of the Campanile</div>
        </div>
    </div>

    <h3>Results: Custom Inpainting on Two Additional Images</h3>
    <p>
        We further experimented with inpainting on two custom images, using different masks to replace selected areas of each image.
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/eiffeltower.jpg" alt="Inpainting Mask for Custom Image 2">
            <div class="caption">Original Image </div>
        </div>
        
        <div class="image-container">
            <img src="images5/part_1_7_altered_eiffell.png" alt="Inpainting Mask for Custom Image 2">
            <div class="caption">Inpainted Image </div>
        </div>
    </div>
    
    <div class="image-row">
        
        <div class="image-container">
            <img src="images5/eye_mask.png" alt="Inpainting Mask for Custom Image 2">
            <div class="caption">Stages</div>
        </div>
        <div class="image-container">
            <img src="images5/part_1_7_altered_eye.png" alt="Inpainted Custom Image 2">
            <div class="caption">Inpainted Image </div>
        </div>
    </div>

        <h2>Part 7.3 Text-Conditioned Image-to-Image Translation</h2>
    <p>
        In this part, we perform image-to-image translation with guidance from a text prompt. By combining the model's inpainting ability with language control, we can adjust the output to match a specific description. This goes beyond simple "projection to the natural image manifold" by incorporating semantic information from the prompt.
    </p>
    <p>
        To achieve this, we modify the prompt from <code>"a high quality photo"</code> to a descriptive phrase, applying different noise levels <code>[1, 3, 5, 7, 10, 20]</code>. The resulting images should maintain characteristics of the original image while aligning with the text prompt.
    </p>

    <h3>Results: Text-Conditioned Edits of the Test Image " a rocket ship"</h3>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/rockettower_1.png" alt="Text-Conditioned Edit at Noise Level 1">
            <div class="caption">Low Noise Levels</div>
        </div>
        <div class="image-container">
            <img src="images5/rockettwoer_2.png" alt="Text-Conditioned Edit at Noise Level 3">
            <div class="caption"> High Noise Levels"</div>
        </div>
        
    </div>
   

    <h3>Results: Text-Conditioned Edits on Custom Images</h3>
    <p>
        We applied the same process to two custom images using the prompt <code>"a photo of a hipster barrista" and "a pencil"</code> and varying noise levels. The outputs show a blend of the original image (a banana) and the characteristics described in the prompt.
    </p>
    <div class="image-row">
        <div class="image-container">
            <img src="images5/bananapencil_1.png" alt="Text-Conditioned Edit on Custom Image 1">
            <div class="caption">Pencil Banana High noise</div>
        </div>
        <div class="image-container">
            <img src="images5/bananapencil_2.png" alt="Text-Conditioned Edit on Custom Image 2">
            <div class="caption">Pencil Banana Low noise</div>
        </div>
    </div>

    <div class="image-row">
        <div class="image-container">
            <img src="images5/baristabanana_1.png" alt="Text-Conditioned Edit on Custom Image 1">
            <div class="caption">Barista Banana High noise</div>
        </div>
        <div class="image-container">
            <img src="images5/baristabanana_2.png" alt="Text-Conditioned Edit on Custom Image 2">
            <div class="caption">Barista Banana Low noise</div>
        </div>
    </div>
    <h2>Part 8 Visual Anagrams</h2>
    <p>
        In this part, we use diffusion models to create visual anagrams—images that show two different scenes when viewed upright and flipped upside down. Our goal is to create an image that appears as <em>"an oil painting of an old man"</em> when viewed normally, and as <em>"an oil painting of people around a campfire"</em> when flipped.
    </p>
    <p>
        To achieve this effect, we perform diffusion denoising on an image <code>x<sub>t</sub></code> at a particular timestep <code>t</code>, with two different prompts. First, we denoise the image using the prompt <em>"an oil painting of an old man"</em>, obtaining a noise estimate <code>ϵ<sub>1</sub></code>. Next, we flip <code>x<sub>t</sub></code> upside down and denoise it with the prompt <em>"an oil painting of people around a campfire"</em>, resulting in <code>ϵ<sub>2</sub></code>. We then flip <code>ϵ<sub>2</sub></code> back to its original orientation and average it with <code>ϵ<sub>1</sub></code> to create the final noise estimate <code>ϵ</code>.
    </p>

    <h3>Algorithm Steps</h3>
    <p>The steps for creating a visual anagram are as follows:</p>
    <ol>
        <li>Generate <code>ϵ<sub>1</sub> = UNet(x<sub>t</sub>, t, p<sub>1</sub>)</code> using the prompt <em>"an oil painting of an old man"</em>.</li>
        <li>Flip <code>x<sub>t</sub></code> upside down and generate <code>ϵ<sub>2</sub> = flip(UNet(flip(x<sub>t</sub>), t, p<sub>2</sub>))</code> using the prompt <em>"an oil painting of people around a campfire"</em>.</li>
        <li>Average the two noise estimates: <code>ϵ = (ϵ<sub>1</sub> + ϵ<sub>2</sub>) / 2</code>.</li>
        <li>Perform a reverse diffusion step with the averaged noise estimate <code>ϵ</code> to obtain the final image.</li>
    </ol>

    <h3>Results: Visual Anagrams</h3>
    <p>
        Below are examples of visual anagrams created using this method. The first image appears as "an oil painting of an old man" when viewed normally and as "an oil painting of people around a campfire" when flipped. We also include two additional anagram illusions that reveal different images when flipped upside down.
    </p>
    
    
    <div class="image-row">
        <div class="image-container">
            <img src="images5/flipped_campfire.png" alt="Visual Anagram - Upright">
            <div class="caption">"an oil painting of an old man" and "an oil painting of people around a campfire"</div>
        </div>
        <div class="image-container">
            <img src="images5/snowy_campfire.png" alt="Visual Anagram - Flipped">
            <div class="caption">"an oil painting of a snowy mountain village" and "an oil painting of people around a campfire"</div>
        </div>
        <div class="image-container">
            <img src="images5/snowy_man.png" alt="Visual Anagram - Flipped">
            <div class="caption">"an oil painting of a snowy mountain village" and "an oil painting of an old man"</div>
        </div>
    </div>

  
    <h2>Part 1.10 Hybrid Images</h2>
    <p>
        In this part, we implement Factorized Diffusion to create hybrid images, inspired by techniques from project 2. A hybrid image combines different elements that appear as one thing when viewed from a distance and something else up close. Here, we use a diffusion model to blend low and high frequencies from two different text prompts, creating a composite effect.
    </p>
    <p>
        The process involves estimating noise using two prompts, combining the low frequencies from one estimate and the high frequencies from the other. This approach allows us to create an image that looks like one subject from afar and transforms into a different subject upon closer inspection.
    </p>

    <h3>Algorithm Steps</h3>
    <p>The steps for generating a hybrid image are as follows:</p>
    <ol>
        <li>Generate the first noise estimate, <code>ϵ<sub>1</sub> = UNet(x<sub>t</sub>, t, p<sub>1</sub>)</code>, with the first prompt.</li>
        <li>Generate the second noise estimate, <code>ϵ<sub>2</sub> = UNet(x<sub>t</sub>, t, p<sub>2</sub>)</code>, with the second prompt.</li>
        <li>Apply a low-pass filter to <code>ϵ<sub>1</sub></code> and a high-pass filter to <code>ϵ<sub>2</sub>:</code> <br>
            <code>ϵ = flowpass(ϵ<sub>1</sub>) + fhighpass(ϵ<sub>2</sub>)</code>.
        </li>
        <li>Use the composite noise estimate <code>ϵ</code> to complete a reverse diffusion step and obtain the final hybrid image.</li>
    </ol>
    <p>
        For this part, we use a Gaussian blur with a kernel size of 33 and sigma of 2 for the low-pass filter, which smooths out the details in the first noise estimate, while the high-pass filter captures the finer details from the second estimate.
    </p>

    <h3>Results: Hybrid Images</h3>
    <p>
        Below are examples of hybrid images created with this technique. The first image appears as a skull from afar and as a waterfall up close. We also include two additional hybrid illusions that transform depending on viewing distance.
    </p>
    
   
    <div class="image-row">
        <div class="image-container">
            <img src="images5/hybrid_skullwaterfall.png" alt="Hybrid Image - Skull / Waterfall">
            <div class="caption">Hybrid Image: Skull from afar, Waterfall up close</div>
        </div>
    

 
    
        <div class="image-container">
            <img src="images5/doghat.png" alt="Hybrid Image - Custom 1">
            <div class="caption">Hybrid Image: A photo of a dog and a man in a hat</div>
        </div>
    
   
        <div class="image-container">
            <img src="images5/hybrid_hatwaterfall.png" alt="Hybrid Image - Custom 2">
            <div class="caption">Hybrid Image: A man with a hat and a waterfall</div>
        </div>
   </div>


    <h3> Final remarks for part A</h3>
    <p>
        I thought this first part of the project was really cool. I think it is very intersting how the images are generated and can't wait for part B which I assume will be even cooler.
        
    </p>

    <h2> Part B</h2>

   <h2>Warm-Up: Building a One-Step Denoiser</h2>
<p>
    To start, we train a simple denoiser <code>Dθ</code> that maps a noisy image <code>x<sub>noisy</sub></code> to a clean image <code>x<sub>clean</sub></code>. The L2 loss is optimized as:
</p>
<blockquote>
    <code>ℒ = ||Dθ(x<sub>noisy</sub>) - x<sub>clean</sub>||²</code>
</blockquote>

<h3>1.1 Implementing the UNet</h3>
<p>
    We use a UNet for the denoiser. It consists of downsampling and upsampling blocks with skip connections, capturing both global and local features. This structure is ideal for image-to-image tasks like denoising.
</p>
<div class="image-container variable-height">
    <img src="images5/Uncond_unet.png" alt="UNet Architecture " style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">UNet Architecture for Denoising</div>
</div>

    <h2>1.2 Denoising Problem and Data Generation</h2>
<p>
    In this part, we implement and visualize the noising process for generating training data. The objective is to train a denoiser <code>Dθ</code> to map noisy images <code>x<sub>noisy</sub></code> to clean images <code>x<sub>clean</sub></code>, minimizing the L2 loss:
</p>
<blockquote>
    <code>ℒ = ||Dθ(x<sub>noisy</sub>) - x<sub>clean</sub>||²</code>
</blockquote>
<p>
    To achieve this, clean MNIST digits <code>x<sub>clean</sub></code> are progressively noised to create training pairs (<code>x<sub>noisy</sub></code>, <code>x<sub>clean</sub></code>).
</p>

<h3>Results: Visualization of the Noising Process</h3>
<p>
    Below is the output of the implemented noising process applied to a normalized MNIST digit. The images show the effect of increasing noise levels:
</p>
<div class="image-container">
    <img src="images5/a_noiselvl.png" alt="Noising Process Visualization" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Progressive noising of a normalized MNIST digit</div>
</div>

    <p>
        For training we used the image class pairs. I used a batchsize of 256, 5 epochs, sigma = 0.5 and the Adam Optimizer with a learning rate of 1e-4. Below you can find
        the loss during training.
    </p>
<div class="image-row">
    <div class="image-container">
    <img src="images5/loss_part1.png" alt="Noising Process Visualization " style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Loss per epoch during training</div>
    </div>

    <div class="image-container">
    <img src="images5/loss_part1_iter.png" alt="Noising Process Visualization" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Loss per iteration during training</div>
    </div>
</div>
    <p>
        The results from the training can be seen below, where I show the performance after 1 epoch and after 5 epochs. As you can see the model becomes much better after 5 epochs.
      
    </p>
<div class="image-row">
<div class="image-container">
    <img src="images5/part1_epoch1.png" alt="Noising Process Visualization" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Results after 1 epoch of training</div>
    </div>

    <div class="image-container">
    <img src="images5/part1_epoch5.png" alt="Noising Process Visualization" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Results after the 5th and final epoch of training</div>
    </div>
</div>

    <p>
        The model was as stated trained using a sigma value of 0.5. But it is intersting to see how well it can perform with higher versus lower values for sigma i.e more and less noise than during training. 
        In the image below you can see that performance.
    </p>

      <div class="image-container">
    <img src="images5/a_diff_sigmas.png" alt="Noising Process Visualization">
    <div class="caption">Results for different values of sigma when trained using 0.5</div>
    </div>


  <h2> Part 2. Time and Class Conditioning</h2>
<p>
    Previously, our UNet model predicted the clean image. In this section, we update the approach to predict the noise <code>ϵ</code> added to the image instead. This allows us to start with pure noise <code>ϵ ∼ N(0, I)</code> and iteratively denoise to generate a realistic image <code>x</code>. 
</p>

<h3>Combining Time and Class Conditioning</h3>
<p>
    Instead of implementing time conditioning first and then adding class conditioning, we simultaneously condition the UNet on both the timestep <code>t</code> and the class of the digit. Using the equation:
</p>
<blockquote>
    <code>x<sub>t</sub> = √ᾱ<sub>t</sub>x<sub>0</sub> + √(1−ᾱ<sub>t</sub>)ϵ, ϵ∼N(0,1)</code>
</blockquote>
<p>
    we generate a noisy image <code>x<sub>t</sub></code> from <code>x<sub>0</sub></code> for a timestep <code>t∈{0,1,…,T}</code>. When <code>t=0</code>, <code>x<sub>t</sub></code> is clean, and when <code>t=T</code>, <code>x<sub>t</sub></code> is pure noise. For intermediate values of <code>t</code>, <code>x<sub>t</sub></code> is a linear combination of the clean image and noise. The derivations for <code>β</code>, <code>α<sub>t</sub></code>, and <code>ᾱ<sub>t</sub></code> follow the DDPM paper, and we set <code>T=400</code> due to the simplicity of our dataset.
</p>

<p>
    To handle time conditioning, we integrate fully connected layers to embed <code>t</code> into the UNet. For class conditioning, we use a one-hot vector to represent each digit class (<code>{0,…,9}</code>). We further add two fully connected layers to embed the class vector. </p>

<h3>Updated UNet Architecture</h3>
<p>
    Below is the updated UNet architecture, which includes both time and class conditioning as well as the new training algorithm used:
</p>
    <div class="image-row">
<div class="image-container variable-height">
    <img src="images5/Timecond_unet.png" alt="Updated UNet Architecture">
    <div class="caption">UNet Architecture with Time and Class Conditioning</div>
</div>
    <div class="image-container variable-height">
    <img src="images5/class_algo.png" alt="Updated UNet Architecture">
    <div class="caption">Training algorithm with Time and Class Conditioning</div>
</div>
    </div>
<h3>Training Algorithm</h3>
<p>
    The training process involves generating noisy images <code>x<sub>t</sub></code> for random timesteps, computing their one-hot class vectors, and training the UNet to predict <code>ϵ</code>. The addition of class conditioning ensures better control over the generated images, while time conditioning enables iterative denoising.
</p>

<h3>Results from Time and Class Conditional UNet wtih Gudienscale: 5.0 </h3>

    </div>
    <div class="image-container variable-height">
    <img src="images5/part2_train_loss.png" alt="Updated UNet Architecture" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Training loss per iteration</div>
</div>
    </div>
    <div class="image-container variable-height">
    <img src="images5/part2_epoch1.png" alt="Updated UNet Architecture">
    <div class="caption">Results after one epoch of training</div>
</div>

    </div>
    <div class="image-container variable-height">
    <img src="images5/part2_epoch5.png" alt="Updated UNet Architecture">
    <div class="caption">Results after five epochs of training</div>
</div>

    </div>
    <div class="image-container variable-height">
    <img src="images5/part2_epoch20.png" alt="Updated UNet Architecture">
    <div class="caption">Results after the final and 20th epoch of training</div>
</div>

    <h3>Sampling with Different Guidance Scales</h3>
<p>
    We sampled images using different guidance scales <code>γ</code>, which balance fidelity to the conditioning signal and diversity. Higher <code>γ</code> values create sharper images but reduce diversity, while lower values allow more randomness.
</p>

<p>
    Below are the generated images with guidance scales <code>γ = [0, 5, 10]</code>:
</p>
<div class="image-container">
    <img src="images5/final_img.png" alt="Sampling with Different Guidance Scales" style="width: 100%; height: auto; border-radius: 0;">
    <div class="caption">Results for guidance scales <code>γ = [0, 5, 10]</code></div>
</div>


      <footer>
        <p>© 2024 Kalle's Portfolio | Project 5: Diffusion Models</p>
        <p>Contact: <a href="mailto:kj00@berkeley.edu">kj00@berkeley.edu</a></p>
    </footer>
</body>
</html>
